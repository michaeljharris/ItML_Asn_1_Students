{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3950 Assignment 1: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Michael Harris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>var_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.138</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  var_1  var_2  var_3  var_4  var_5  var_6  var_7  var_8  var_9  \\\n",
       "162       1  0.814  0.195  0.511  0.121  0.804  0.650  0.114  0.654  0.428   \n",
       "224       1  0.729  0.445  0.775  0.286  0.441  0.345  0.216  0.790  0.697   \n",
       "71        0  0.295  0.129  0.070  0.859  0.849  0.248  0.629  0.690  0.084   \n",
       "119       0  0.679  0.345  0.321  0.411  0.279  0.700  0.046  0.117  0.762   \n",
       "19        0  0.088  0.813  0.837  0.104  0.478  0.264  0.868  0.110  0.406   \n",
       "\n",
       "     ...  var_191  var_192  var_193  var_194  var_195  var_196  var_197  \\\n",
       "162  ...    0.405    0.908    0.786    0.430    0.350    0.974    0.134   \n",
       "224  ...    0.077    0.363    0.005    0.018    0.850    0.682    0.390   \n",
       "71   ...    0.488    0.911    0.526    0.466    0.979    0.378    0.551   \n",
       "119  ...    0.666    0.306    0.552    0.754    0.013    0.581    0.030   \n",
       "19   ...    0.930    0.535    0.799    0.207    0.074    0.138    0.138   \n",
       "\n",
       "     var_198  var_199  var_200  \n",
       "162    0.002    0.039    0.940  \n",
       "224    0.086    0.223    0.077  \n",
       "71     0.356    0.659    0.532  \n",
       "119    0.662    0.516    0.641  \n",
       "19     1.000    0.516    0.599  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"training.csv\")\n",
    "df = df.drop(columns={\"id\"})\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing and training data\n",
    "df2 = pd.get_dummies(df, drop_first=True)\n",
    "y = np.array(df2[\"target\"]).reshape(-1,1)\n",
    "X = np.array(df2.drop(columns={\"target\"}))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('forest',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=9,\n",
      "                                        min_samples_leaf=6, min_samples_split=5,\n",
      "                                        n_estimators=85, n_jobs=-1))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create model using standard scalar and a forest in a pipeline\n",
    "scaler = StandardScaler()\n",
    "estimator = RandomForestClassifier(n_jobs=-1)\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"forest\", estimator)])\n",
    "\n",
    "params = {'forest__max_depth':[6,7,8,9],\n",
    "          'forest__min_samples_split':[3,4,5,6,7],\n",
    "          'forest__min_samples_leaf' :[2,3,4,5,6],\n",
    "          'forest__n_estimators' :[75,80,85],\n",
    "          'forest__criterion':[\"entropy\"]}\n",
    " \n",
    "best = GridSearchCV(pipe, param_grid=params, cv=5, n_jobs=-1) \n",
    "best.fit(X_train, y_train.ravel())\n",
    "best1 = best.best_estimator_\n",
    "print(best1.score(X_test, y_test))\n",
    "print(best1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6031746031746031\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('forest',\n",
      "                                        RandomForestClassifier(n_jobs=-1))]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'forest__criterion': ['entropy'],\n",
      "                         'forest__max_depth': [6, 7, 8, 9],\n",
      "                         'forest__min_samples_leaf': [2, 3, 4, 5, 6],\n",
      "                         'forest__min_samples_split': [3, 4, 5, 6, 7],\n",
      "                         'forest__n_estimators': [75, 80, 85]})\n"
     ]
    }
   ],
   "source": [
    "print(best.score(X_test, y_test))\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6396708010582511\n",
      "0.6394430379746835\n",
      "Michael Harris 0.6395569195164673\n"
     ]
    }
   ],
   "source": [
    "#Load Test Data\n",
    "test_df = pd.read_csv(\"testing.csv\")\n",
    "test_df = test_df.drop(columns={\"id\"})\n",
    "#Create tests and score\n",
    "test_y = np.array(test_df[\"target\"]).reshape(-1,1)\n",
    "test_X = np.array(test_df.drop(columns={\"target\"}))\n",
    "\n",
    "preds = best.predict(test_X)\n",
    "\n",
    "roc_score = roc_auc_score(test_y, preds)\n",
    "acc_score = accuracy_score(test_y, preds)\n",
    "\n",
    "print(roc_score)\n",
    "print(acc_score)\n",
    "print(name, np.mean([roc_score, acc_score]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Accuracy Changes Were Used\n",
    "\n",
    "Please list here what you did to try to increase accuracy and/or limit overfitting:\n",
    "<ul>\n",
    "<li> For the model I used a forest classifier instead of a single tree, in combination with a standard scalar in a pipeline.\n",
    "<li> For that forest model I used a grid search with a combination of four parameters: max depth, minimum sample split, minimum sample leaf and n estimators. I also set the criterion to entropy.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
